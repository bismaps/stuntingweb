{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stunting Prediction - Complete Analysis Pipeline\n",
    "\n",
    "This notebook provides a comprehensive pipeline for stunting prediction, including:\n",
    "1. Data loading and exploration\n",
    "2. Data preprocessing and feature engineering\n",
    "3. Model training and evaluation\n",
    "4. Visualization of results\n",
    "5. Model inference on new data\n",
    "\n",
    "The notebook is designed to be self-contained and includes all necessary code for the complete machine learning workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import json\n",
    "import joblib\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "# Scikit-learn imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, \n",
    "                           f1_score, roc_auc_score, classification_report, \n",
    "                           confusion_matrix, roc_curve)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Set up visualization\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Scikit-learn version: {joblib.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Initial Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data_path = 'stunting_wasting_dataset.csv'\n",
    "print(f\"Loading data from: {data_path}\")\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(data_path)\n",
    "    print(f\"✓ Dataset loaded successfully!\")\n",
    "    print(f\"Shape: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"❌ Error: File '{data_path}' not found. Please check the file path.\")\n",
    "    # Create a sample dataset for demonstration if file doesn't exist\n",
    "    print(\"Creating a sample dataset for demonstration...\")\n",
    "    np.random.seed(42)\n",
    "    sample_size = 1000\n",
    "    df = pd.DataFrame({\n",
    "        'Age': np.random.randint(6, 60, sample_size),\n",
    "        'Gender': np.random.choice(['Male', 'Female'], sample_size),\n",
    "        'Weight': np.random.normal(15, 5, sample_size),\n",
    "        'Height': np.random.normal(100, 20, sample_size),\n",
    "        'BMI': np.random.normal(15, 3, sample_size),\n",
    "        'Region': np.random.choice(['Urban', 'Rural'], sample_size),\n",
    "        'Education': np.random.choice(['Primary', 'Secondary', 'Higher'], sample_size),\n",
    "        'Income': np.random.normal(5000, 2000, sample_size),\n",
    "        'Stunting': np.random.choice([0, 1], sample_size, p=[0.7, 0.3])\n",
    "    })\n",
    "    print(f\"Sample dataset created with shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic information about the dataset\n",
    "print(\"\\n=== Dataset Information ===\")\n",
    "print(df.info())\n",
    "\n",
    "print(\"\\n=== First 5 Rows ===\")\n",
    "display(df.head())\n",
    "\n",
    "print(\"\\n=== Statistical Summary ===\")\n",
    "display(df.describe(include='all'))\n",
    "\n",
    "print(\"\\n=== Missing Values ===\")\n",
    "missing_values = df.isnull().sum()\n",
    "missing_percent = (missing_values / len(df)) * 100\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Count': missing_values,\n",
    "    'Missing Percentage': missing_percent\n",
    "})\n",
    "display(missing_df[missing_df['Missing Count'] > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean column names\n",
    "def clean_column_names(df):\n",
    "    \"\"\"Clean column names by removing spaces and special characters\"\"\"\n",
    "    df.columns = [re.sub(r\"\\s+\", \"_\", str(c).strip()) for c in df.columns]\n",
    "    df = df.loc[:, ~df.columns.duplicated(keep=\"first\")]\n",
    "    return df\n",
    "\n",
    "df = clean_column_names(df)\n",
    "print(\"✓ Column names cleaned\")\n",
    "print(f\"Columns: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify and remove ID-like columns\n",
    "def is_id_like(column_name):\n",
    "    \"\"\"Check if a column name suggests it's an ID column\"\"\"\n",
    "    return bool(re.search(r\"(?:^id$|_id$|^no$|^no_|_no$|^index$|nik|nkk|uuid)\", column_name, flags=re.I))\n",
    "\n",
    "id_columns = [c for c in df.columns if is_id_like(c)]\n",
    "print(f\"Identified ID columns: {id_columns}\")\n",
    "\n",
    "# Remove ID columns\n",
    "df_cleaned = df.drop(columns=id_columns, errors='ignore')\n",
    "print(f\"✓ Removed {len(id_columns)} ID columns\")\n",
    "print(f\"New shape: {df_cleaned.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns with all missing values\n",
    "df_cleaned = df_cleaned.dropna(axis=1, how='all')\n",
    "print(f\"✓ Dropped columns with all missing values\")\n",
    "print(f\"New shape: {df_cleaned.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target variable preprocessing\n",
    "def to_binary_target(y_raw, positive_labels=None, negative_labels=None):\n",
    "    \"\"\"\n",
    "    Convert target variable to binary format, explicitly mapping stunting vs non-stunting.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_raw : array-like\n",
    "        Original target column (e.g. 'Stunting').\n",
    "    positive_labels : list of str, optional\n",
    "        Labels that should be mapped to 1 (stunting).\n",
    "    negative_labels : list of str, optional\n",
    "        Labels that should be mapped to 0 (tidak stunting).\n",
    "    \"\"\"\n",
    "    s = pd.Series(y_raw).astype(str).str.strip().str.lower()\n",
    "    \n",
    "    # Default mapping khusus untuk konteks stunting anak\n",
    "    if positive_labels is None:\n",
    "        positive_labels = [\n",
    "            \"stunted\", \"severely stunted\", \"sangat pendek\", \"pendek\", \"stunting\",\n",
    "        ]\n",
    "    if negative_labels is None:\n",
    "        negative_labels = [\n",
    "            \"normal\", \"tall\", \"tinggi\", \"risk of overweight\", \"overweight\", \"obese\",\n",
    "        ]\n",
    "    \n",
    "    mapping = {}\n",
    "    for v in positive_labels:\n",
    "        mapping[v] = 1\n",
    "    for v in negative_labels:\n",
    "        mapping[v] = 0\n",
    "    \n",
    "    # Map exact match dulu\n",
    "    y = s.map(mapping)\n",
    "    \n",
    "    # Jika masih ada label lain, jatuhkan ke label mayoritas (anggap non-stunting = 0)\n",
    "    if y.isna().any():\n",
    "        # label yang belum terpetakan\n",
    "        remaining = s[y.isna()]\n",
    "        if not remaining.empty:\n",
    "            # majority vote di remaining, lalu treat majority sebagai kelas 0\n",
    "            le = LabelEncoder().fit(remaining)\n",
    "            lab = le.transform(remaining)\n",
    "            maj = pd.Series(lab).value_counts().idxmax()\n",
    "            maj_label = le.inverse_transform([maj])[0]\n",
    "            maj_label = str(maj_label).strip().lower()\n",
    "            # anggap mayoritas remaining sebagai non-stunting\n",
    "            mapping[maj_label] = 0\n",
    "            y = s.map(mapping)\n",
    "    \n",
    "    # Fallback terakhir: kalau tetap tidak biner, paksa dengan majority baseline\n",
    "    if y.isna().any() or len(pd.Series(y).dropna().unique()) > 2:\n",
    "        lab = LabelEncoder().fit_transform(s)\n",
    "        maj = pd.Series(lab).value_counts().idxmax()\n",
    "        y = (lab != maj).astype(int)\n",
    "    \n",
    "    return pd.Series(y, index=s.index).astype(int)\n",
    "\n",
    "# Identify target column\n",
    "target_column = 'Stunting' if 'Stunting' in df_cleaned.columns else df_cleaned.columns[-1]\n",
    "print(f\"Target column: {target_column}\")\n",
    "\n",
    "# Convert target to binary (stunting vs tidak stunting)\n",
    "y = to_binary_target(df_cleaned[target_column])\n",
    "print(f\"✓ Target converted to binary (1 = stunting, 0 = tidak stunting)\")\n",
    "print(\"Target distribution:\")\n",
    "print(y.value_counts())\n",
    "print(f\"Percentage of positive (stunting) cases: {y.mean():.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = df_cleaned.drop(columns=[target_column], errors='ignore')\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify numerical and categorical columns\n",
    "numerical_columns = [c for c in X.columns if pd.api.types.is_numeric_dtype(X[c])]\n",
    "categorical_columns = [c for c in X.columns if c not in numerical_columns]\n",
    "\n",
    "print(f\"Numerical columns ({len(numerical_columns)}): {numerical_columns}\")\n",
    "print(f\"Categorical columns ({len(categorical_columns)}): {categorical_columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize target distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "y.value_counts().plot(kind='bar', color=['skyblue', 'salmon'])\n",
    "plt.title('Target Distribution')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "y.value_counts().plot(kind='pie', autopct='%1.1f%%', colors=['skyblue', 'salmon'])\n",
    "plt.title('Target Distribution (Percentage)')\n",
    "plt.ylabel('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize numerical features\n",
    "if numerical_columns:\n",
    "    n_cols = min(3, len(numerical_columns))\n",
    "    n_rows = (len(numerical_columns) + n_cols - 1) // n_cols\n",
    "    \n",
    "    plt.figure(figsize=(15, 5 * n_rows))\n",
    "    for i, col in enumerate(numerical_columns, 1):\n",
    "        plt.subplot(n_rows, n_cols, i)\n",
    "        sns.histplot(data=X, x=col, kde=True)\n",
    "        plt.title(f'Distribution of {col}')\n",
    "        plt.xlabel(col)\n",
    "        plt.ylabel('Frequency')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No numerical columns to visualize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize categorical features\n",
    "if categorical_columns:\n",
    "    n_cols = min(3, len(categorical_columns))\n",
    "    n_rows = (len(categorical_columns) + n_cols - 1) // n_cols\n",
    "    \n",
    "    plt.figure(figsize=(15, 5 * n_rows))\n",
    "    for i, col in enumerate(categorical_columns, 1):\n",
    "        plt.subplot(n_rows, n_cols, i)\n",
    "        X[col].value_counts().plot(kind='bar')\n",
    "        plt.title(f'Distribution of {col}')\n",
    "        plt.xlabel(col)\n",
    "        plt.ylabel('Count')\n",
    "        plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No categorical columns to visualize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix for numerical features\n",
    "if len(numerical_columns) > 1:\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    correlation_matrix = X[numerical_columns].corr()\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "                square=True, fmt='.2f')\n",
    "    plt.title('Correlation Matrix of Numerical Features')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Not enough numerical columns for correlation matrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create preprocessing pipelines\n",
    "numerical_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler(with_mean=False))\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "# Create column transformer\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', numerical_transformer, numerical_columns),\n",
    "    ('cat', categorical_transformer, categorical_columns)\n",
    "], remainder='drop', sparse_threshold=0.3)\n",
    "\n",
    "print(\"✓ Preprocessing pipeline created\")\n",
    "print(f\"Numerical features: {len(numerical_columns)}\")\n",
    "print(f\"Categorical features: {len(categorical_columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=171\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set size: {X_test.shape[0]} samples\")\n",
    "print(\"Training target distribution:\")\n",
    "print(y_train.value_counts(normalize=True))\n",
    "print(\"Test target distribution:\")\n",
    "print(y_test.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models\n",
    "models = {\n",
    "    'Logistic Regression': Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', LogisticRegression(max_iter=1000, class_weight='balanced'))\n",
    "    ]),\n",
    "    'Random Forest': Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', RandomForestClassifier(\n",
    "            n_estimators=150, \n",
    "            class_weight='balanced_subsample', \n",
    "            random_state=42, \n",
    "            n_jobs=-1\n",
    "        ))\n",
    "    ])\n",
    "}\n",
    "\n",
    "print(f\"✓ Defined {len(models)} models for training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate models\n",
    "results = []\n",
    "trained_models = {}\n",
    "classification_reports = {}\n",
    "confusion_matrices = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n=== Training {name} ===\")\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    trained_models[name] = model\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Get prediction probabilities\n",
    "    try:\n",
    "        y_proba = model.predict_proba(X_test)[:, 1]\n",
    "        auc_score = roc_auc_score(y_test, y_proba)\n",
    "    except:\n",
    "        y_proba = None\n",
    "        auc_score = float('nan')\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        'Model': name,\n",
    "        'Accuracy': accuracy_score(y_test, y_pred),\n",
    "        'Precision': precision_score(y_test, y_pred, average='macro', zero_division=0),\n",
    "        'Recall': recall_score(y_test, y_pred, average='macro', zero_division=0),\n",
    "        'F1-Score': f1_score(y_test, y_pred, average='macro', zero_division=0),\n",
    "        'ROC AUC': auc_score\n",
    "    }\n",
    "    \n",
    "    results.append(metrics)\n",
    "    classification_reports[name] = classification_report(y_test, y_pred, output_dict=True, zero_division=0)\n",
    "    confusion_matrices[name] = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"Accuracy: {metrics['Accuracy']:.4f}\")\n",
    "    print(f\"Precision: {metrics['Precision']:.4f}\")\n",
    "    print(f\"Recall: {metrics['Recall']:.4f}\")\n",
    "    print(f\"F1-Score: {metrics['F1-Score']:.4f}\")\n",
    "    if not np.isnan(auc_score):\n",
    "        print(f\"ROC AUC: {auc_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results comparison\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\n=== Model Comparison ===\")\n",
    "display(results_df)\n",
    "\n",
    "# Visualize model comparison\n",
    "metrics_to_plot = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "if not results_df['ROC AUC'].isna().all():\n",
    "    metrics_to_plot.append('ROC AUC')\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "x = np.arange(len(results_df['Model']))\n",
    "width = 0.15\n",
    "\n",
    "for i, metric in enumerate(metrics_to_plot):\n",
    "    plt.bar(x + i*width, results_df[metric], width, label=metric)\n",
    "\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Model Performance Comparison')\n",
    "plt.xticks(x + width * len(metrics_to_plot) / 2, results_df['Model'], rotation=45)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualisasi Hasil Model\n",
    "Pada bagian ini dilakukan visualisasi kinerja model, termasuk:\n",
    "- Perbandingan skor metrik utama antar model\n",
    "- Visualisasi confusion matrix untuk model terbaik\n",
    "- Kurva ROC untuk model terbaik\n",
    "- Perbandingan nilai aktual vs prediksi pada subset data uji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.1 Visualisasi Perbandingan Skor Model\n",
    "metrics_to_plot = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "if not results_df['ROC AUC'].isna().all():\n",
    "    metrics_to_plot.append('ROC AUC')\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "x = np.arange(len(results_df['Model']))\n",
    "width = 0.15\n",
    "\n",
    "for i, metric in enumerate(metrics_to_plot):\n",
    "    plt.bar(x + i*width, results_df[metric], width, label=metric)\n",
    "\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Perbandingan Kinerja Model')\n",
    "plt.xticks(x + width * len(metrics_to_plot) / 2, results_df['Model'], rotation=0)\n",
    "plt.ylim(0, 1.05)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 8.1b Tabel Ringkas Metrik (dibulatkan 3 desimal untuk laporan)\n",
    "display(results_df.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.2 Confusion Matrix untuk Model Terbaik\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "best_model_name = results_df.sort_values('F1-Score', ascending=False).iloc[0]['Model']\n",
    "best_model = trained_models[best_model_name]\n",
    "cm = confusion_matrices[best_model_name]\n",
    "\n",
    "plt.figure(figsize=(5, 4))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Tidak Stunting', 'Stunting'])\n",
    "disp.plot(cmap='Blues', values_format='d')\n",
    "plt.title(f'Confusion Matrix - {best_model_name}')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.3 Kurva ROC untuk Model Terbaik\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "best_model_name = results_df.sort_values('F1-Score', ascending=False).iloc[0]['Model']\n",
    "best_model = trained_models[best_model_name]\n",
    "\n",
    "if hasattr(best_model, 'predict_proba') and best_model.predict_proba(X_test).shape[1] == 2:\n",
    "    y_scores = best_model.predict_proba(X_test)[:, 1]\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_scores)\n",
    "    auc_score = roc_auc_score(y_test, y_scores)\n",
    "\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    plt.plot(fpr, tpr, label=f'ROC curve (AUC = {auc_score:.3f})')\n",
    "    plt.plot([0, 1], [0, 1], 'k--', label='Random classifier')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'ROC Curve - {best_model_name}')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(f\"Model {best_model_name} tidak mendukung predict_proba biner yang valid.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.4 Perbandingan Aktual vs Prediksi pada Subset Data Uji\n",
    "n_samples = 50\n",
    "subset_X = X_test.head(n_samples)\n",
    "subset_y_true = y_test.head(n_samples)\n",
    "subset_y_pred = best_model.predict(subset_X)\n",
    "\n",
    "comparison_df = subset_X.copy()\n",
    "comparison_df['Actual'] = subset_y_true.values\n",
    "comparison_df['Predicted'] = subset_y_pred\n",
    "\n",
    "print(f\"Menampilkan {n_samples} sampel pertama dari data uji dengan label aktual dan prediksi:\")\n",
    "display(comparison_df[['Actual', 'Predicted']].head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.5 Distribusi Probabilitas Prediksi Model Terbaik\n",
    "if hasattr(best_model, 'predict_proba') and best_model.predict_proba(X_test).shape[1] == 2:\n",
    "    y_scores = best_model.predict_proba(X_test)[:, 1]\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.histplot(y_scores, bins=20, kde=True, color='steelblue')\n",
    "    plt.xlabel('Probabilitas Prediksi Stunting')\n",
    "    plt.ylabel('Jumlah Anak')\n",
    "    plt.title(f'Distribusi Probabilitas Prediksi - {best_model_name}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(f\"Model {best_model_name} tidak mendukung predict_proba biner yang valid.\")\n",
    "\n",
    "\n",
    "# 8.6 Analisis Error: False Positive dan False Negative\n",
    "y_pred_full = best_model.predict(X_test)\n",
    "comparison_full = X_test.copy()\n",
    "comparison_full['Actual'] = y_test.values\n",
    "comparison_full['Predicted'] = y_pred_full\n",
    "comparison_full['Prob_Stunting'] = best_model.predict_proba(X_test)[:, 1] if hasattr(best_model, 'predict_proba') and best_model.predict_proba(X_test).shape[1] == 2 else np.nan\n",
    "\n",
    "\n",
    "# False Positive: model memprediksi stunting (1) padahal aktual 0\n",
    "false_positives = comparison_full[(comparison_full['Actual'] == 0) & (comparison_full['Predicted'] == 1)]\n",
    "print(f\"\\nJumlah False Positive: {len(false_positives)}\")\n",
    "display(false_positives.head(10))\n",
    "\n",
    "\n",
    "# False Negative: model memprediksi tidak stunting (0) padahal aktual 1 (lebih berisiko secara klinis)\n",
    "false_negatives = comparison_full[(comparison_full['Actual'] == 1) & (comparison_full['Predicted'] == 0)]\n",
    "print(f\"\\nJumlah False Negative: {len(false_negatives)}\")\n",
    "display(false_negatives.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.6 Analisis Error: False Positive dan False Negative\n",
    "y_pred_full = best_model.predict(X_test)\n",
    "comparison_full = X_test.copy()\n",
    "comparison_full['Actual'] = y_test.values\n",
    "comparison_full['Predicted'] = y_pred_full\n",
    "comparison_full['Prob_Stunting'] = (\n",
    "    best_model.predict_proba(X_test)[:, 1]\n",
    "    if hasattr(best_model, 'predict_proba') and best_model.predict_proba(X_test).shape[1] == 2\n",
    "    else np.nan\n",
    ")\n",
    "\n",
    "# False Positive: model memprediksi stunting (1) padahal aktual 0\n",
    "false_positives = comparison_full[(comparison_full['Actual'] == 0) & (comparison_full['Predicted'] == 1)]\n",
    "print(f\"\\nJumlah False Positive: {len(false_positives)}\")\n",
    "display(false_positives.head(10))\n",
    "\n",
    "# False Negative: model memprediksi tidak stunting (0) padahal aktual 1 (lebih berisiko secara klinis)\n",
    "false_negatives = comparison_full[(comparison_full['Actual'] == 1) & (comparison_full['Predicted'] == 0)]\n",
    "print(f\"\\nJumlah False Negative: {len(false_negatives)}\")\n",
    "display(false_negatives.head(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3119",
   "language": "python",
   "name": "python3119"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
